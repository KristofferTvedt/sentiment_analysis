---
title: "Sentiment Analysis EU commission AI act"
author: "Kristoffer Bakke Tvedt"
date: "2024-07-06"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.textmodels)
library(tidyverse)
library(remotes)
library(quanteda.sentiment)
```

Reads the RDS file created from pdf files and filters out any papers that are not by  Business or NGO
```{r}
df <- readRDS("data_corpus_aiact_english_new.rds")
df_business_ngo <- df%>%
  filter(type_actor %in% c("Business", "NGO"))
```

Creates a corpus from df and changes docvar Names to the actor of the paper. Displays a summary of said corpus. 
```{r}
original_names <- df_business_ngo$actor
actor_types <- df_business_ngo$type_actor
data_corpus_aiact <- corpus(df_business_ngo)
docnames(data_corpus_aiact) <- paste(original_names, actor_types, sep = " - ")
summary(data_corpus_aiact)
```

Creates a dfm from token object
```{r}
toks_aiact2 <- tokens(data_corpus_aiact, remove_punct = T, remove_numbers = T) %>%
tokens_remove(stopwords("en")) %>%
dfm()
toks_aiact2
```
Shows most frequent terms by document
```{r}
textstat_frequency(toks_aiact2, n = 10, groups = actor)
```

Groups the dfm by type_actor
```{r}
toks_aiact3 <- toks_aiact2 %>%
dfm_group(groups = type_actor)
```

Keyness analysis
```{r}
tstat_key <- textstat_keyness(toks_aiact3, measure = "chi2")
tplot_key <- textplot_keyness(tstat_key, n = 20)
tplot_key
```

token object based on the first token object
```{r}
toks_aiact4 <- toks_aiact %>%
tokens_remove(stopwords("en"), padding = T )
```

Collocation analysis
```{r}
col_aiact <- textstat_collocations(toks_aiact4, size = 2:3)
```

Looks for multiword expressions and creates a new token object from this
```{r}
toks_aiact5 <- tokens(col_aiact$collocation)
toks_aiact6 <- tokens(toks_aiact4) %>%
tokens_compound(pattern = as.phrase(toks_aiact5))
toks_aiact6
```

Removes punctuation characters, punctuation, and symbols from previous token object and trim it by including only terms that appear at least three times
```{r}
toks_aiact7 <- tokens(toks_aiact6, remove_punct = T, remove_symbols = T) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 3)
toks_aiact7
```

Wordfish analysis
```{r}
aiact_wf <- textmodel_wordfish(toks_aiact7, dir = c(6,5))
textplot_scale1d(aiact_wf, groups = aiact_wf$type_actor)
summary(aiact_wf)
```

Creates new object and applies Lexicoder Sentiment Dictionary
```{r}
toks_lsd <- toks_aiact %>%
  tokens_lookup(dictionary = data_dictionary_LSD2015) %>%
  dfm()
```

gets sentiment score
```{r}
aiact_ss <- textstat_polarity(toks_lsd, data_dictionary_LSD2015)
aiact_ss$type_actor <- toks_lsd$type_actor
aiact_ss
```

Creates new variable with only Business and NGO sentiment score to measure mean and median
```{r}
aiact_ss_business <- aiact_ss %>%
  filter(type_actor %in% c("Business"))
aiact_ss_ngo <- aiact_ss %>%
  filter(type_actor %in% c("NGO"))
mean(aiact_ss_business$sentiment)
median(aiact_ss_business$sentiment)
mean(aiact_ss_ngo$sentiment)
median(aiact_ss_ngo$sentiment)
```

plots the sentiment scores
```{r}
ggplot(data = aiact_ss, aes(x=sentiment, y=doc_id, color = type_actor, shape = type_actor)) + geom_point() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5))+ labs(x="Sentiment score", y="Documents")+ theme(axis.text.y = element_text(size = 5))
```

Gets the 50 most used postitive words
```{r}
toks_aiact8 <- tokens(data_corpus_aiact) %>%
tokens_keep(pattern = data_dictionary_LSD2015$positive) %>%
  dfm()
topfeatures(toks_aiact8, n=50)
```

Gets the 50 most used negative words
```{r}
toks_aiact9 <- tokens(data_corpus_aiact) %>%
tokens_keep(pattern = data_dictionary_LSD2015$negative) %>%
  dfm()
topfeatures(toks_aiact9, n=50)
```